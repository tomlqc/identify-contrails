{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Contrails with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reinstall tensorflow-io\n",
    "# to avoid the UserWarning: unable to load libtensorflow_io_plugins.so\n",
    "\n",
    "#!pip install tensorflow-io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLATFORM = gcp\n",
      "\n",
      "LOAD_CHECKPOINT = False\n",
      "TRAIN = True\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "\n",
    "LOAD_CHECKPOINT = False  # <DEVEL>\n",
    "TRAIN = True  # <DEVEL>\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "    PLATFORM = 'kaggle'\n",
    "else:\n",
    "    PLATFORM = 'gcp'\n",
    "\n",
    "# ==============================\n",
    "\n",
    "print(f'PLATFORM = {PLATFORM}')\n",
    "print()\n",
    "print(f'LOAD_CHECKPOINT = {LOAD_CHECKPOINT}')\n",
    "print(f'TRAIN = {TRAIN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/kaggle/working\n",
      "PWD = /home/jupyter/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "if PLATFORM == 'kaggle':\n",
    "\n",
    "    WORK_DIR = '/kaggle/working'  # preserved if notebook is saved\n",
    "    TEMP_DIR = '/kaggle/temp'  # just during current session\n",
    "\n",
    "    DATA_DIR = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n",
    "    \n",
    "    WEIGHTS_DIR = WORK_DIR\n",
    "    \n",
    "    resnet50_imagenet_weights =\\\n",
    "        '/kaggle/input/d/alexisbcook/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "    # You can write up to 20GB to the current directory (/kaggle/working/)\n",
    "    # that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "    # You can also write temporary files to /kaggle/temp/,\n",
    "    # but they won't be saved outside of the current session\n",
    "\n",
    "elif PLATFORM == 'gcp':\n",
    "\n",
    "    WORK_DIR = '/home/jupyter/kaggle/working'  # preserved if notebook is saved\n",
    "    TEMP_DIR = '/home/jupyter/kaggle/temp'  # just during current session\n",
    "\n",
    "    DATA_DIR = '/home/jupyter/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n",
    "    \n",
    "    WEIGHTS_DIR = '/home/jupyter/identify-contrails-models'\n",
    "    \n",
    "    resnet50_imagenet_weights = 'imagenet'\n",
    "    \n",
    "    %cd $WORK_DIR\n",
    "    \n",
    "else:\n",
    "    raise NotImplementedError(f'unknown platform \"{PLATFORM}\"')\n",
    "\n",
    "print('PWD =', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_DATA = False\n",
    "\n",
    "if UPDATE_DATA:\n",
    "    %cp -v /kaggle/input/identify-contrails/identify-contrails_2023*.h5 .\n",
    "    %ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CHECKPOINT:\n",
    "    prev_checkpoint_path = os.path.join(WEIGHTS_DIR, 'identify-contrails_2023-07-22_16-22-01.h5')  # <DEVEL>\n",
    "    print(f'prev_checkpoint_path = {prev_checkpoint_path}')\n",
    "    if not os.path.exists(prev_checkpoint_path):\n",
    "        raise IOError(f'file does not exist {prev_checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 818M\n",
      "-rw-r--r-- 1 jupyter 137M Jul 24 17:16 identify-contrails_2023-07-24_18-25-59.h5\n",
      "-rw-r--r-- 1 jupyter  997 Jul 24 17:16 identify-contrails_2023-07-24_18-25-59_log.csv\n",
      "-rw-r--r-- 1 jupyter 137M Jul 24 19:46 identify-contrails_2023-07-24_21-10-06.h5\n",
      "-rw-r--r-- 1 jupyter 1003 Jul 24 20:01 identify-contrails_2023-07-24_21-10-06_log.csv\n",
      "-rw-r--r-- 1 jupyter 137M Jul 24 21:19 identify-contrails_2023-07-24_22-38-16.h5\n",
      "-rw-r--r-- 1 jupyter  881 Jul 24 21:29 identify-contrails_2023-07-24_22-38-16_log.csv\n",
      "-rw-r--r-- 1 jupyter 137M Jul 24 22:31 identify-contrails_2023-07-24_23-40-45.h5\n",
      "-rw-r--r-- 1 jupyter  466 Jul 24 22:31 identify-contrails_2023-07-24_23-40-45_log.csv\n",
      "-rw-r--r-- 1 jupyter 137M Jul 24 22:52 identify-contrails_2023-07-25_00-42-00.h5\n",
      "-rw-r--r-- 1 jupyter  214 Jul 24 22:52 identify-contrails_2023-07-25_00-42-00_log.csv\n",
      "-rw-r--r-- 1 jupyter 137M Jul 25 00:54 identify-contrails_2023-07-25_00-58-50.h5\n",
      "-rw-r--r-- 1 jupyter 4.8K Jul 25 03:08 identify-contrails_2023-07-25_00-58-50_log.csv\n",
      "-rw------- 1 jupyter  25K Jul 25 03:11 nohup.out\n",
      "-rw-r--r-- 1 jupyter   69 Jul 25 10:10 submission.csv\n"
     ]
    }
   ],
   "source": [
    "!touch submission.csv\n",
    "%ll -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import datetime\n",
    "import itertools\n",
    "import math\n",
    "import multiprocessing\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from pprint import pprint\n",
    "from pytz import timezone\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 2023-07-25 12:10:42 CEST+0200\n"
     ]
    }
   ],
   "source": [
    "PRINT_TIME_FORMAT = \"%Y-%m-%d %H:%M:%S %Z%z\"\n",
    "FILE_TIME_FORMAT = \"%Y-%m-%d_%H-%M-%S\"\n",
    "\n",
    "start_time = datetime.datetime.now(timezone('CET'))\n",
    "\n",
    "file_time_str = start_time.strftime(FILE_TIME_FORMAT)\n",
    "\n",
    "print('Started', start_time.strftime(PRINT_TIME_FORMAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 10:10:42.315372: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-25 10:10:45.681416: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-07-25 10:10:45.683663: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-07-25 10:10:45.683676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as backend\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num CPUs Available:  1\n",
      "Num GPUs Available:  1\n",
      "cpu_count:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 10:10:48.928866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-25 10:10:49.184493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-25 10:10:49.184761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print('cpu_count: ', multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paths:\n",
    "    train = os.path.join(DATA_DIR, 'train')\n",
    "    valid = os.path.join(DATA_DIR, 'validation')\n",
    "    test = os.path.join(DATA_DIR, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples (train, validation, test) = 20529 1856 2\n"
     ]
    }
   ],
   "source": [
    "train_ids = sorted(os.listdir(Paths.train))\n",
    "valid_ids = sorted(os.listdir(Paths.valid))\n",
    "test_ids = sorted(os.listdir(Paths.test))\n",
    "print('n_samples (train, validation, test) =', len(train_ids), len(valid_ids), len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABI:\n",
    "    bands = {name: idx for idx, name in enumerate([\n",
    "        '08', '09', '10', '11', '12', '13', '14', '15', '16'])}\n",
    "    colors = {name: idx for idx, name in enumerate([\n",
    "        'red', 'blue', 'green', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TIMES_BEFORE = 4\n",
    "N_TIMES_AFTER = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_range(data, bounds):\n",
    "    \"\"\"Maps data to the range [0, 1].\"\"\"\n",
    "    return (data - bounds[0]) / (bounds[1] - bounds[0])\n",
    "\n",
    "_T11_BOUNDS = (243, 303)\n",
    "_CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n",
    "_TDIFF_BOUNDS = (-4, 2)\n",
    "\n",
    "def get_ash_colors(sample_id, split_dir):\n",
    "    \"\"\"\n",
    "    Based on bands: 11, 14, 15\n",
    "    \n",
    "    Args:\n",
    "        sample_id(str): The id of the example i.e. '1000216489776414077'\n",
    "        split_dir(str): The split directoryu i.e. 'test', 'train', 'val'\n",
    "    \"\"\"\n",
    "    band15 = np.load(DATA_DIR + f\"/{split_dir}/{sample_id}/band_15.npy\")\n",
    "    band14 = np.load(DATA_DIR + f\"/{split_dir}/{sample_id}/band_14.npy\")\n",
    "    band11 = np.load(DATA_DIR + f\"/{split_dir}/{sample_id}/band_11.npy\")\n",
    "\n",
    "    r = normalize_range(band15 - band14, _TDIFF_BOUNDS)\n",
    "    g = normalize_range(band14 - band11, _CLOUD_TOP_TDIFF_BOUNDS)\n",
    "    b = normalize_range(band14, _T11_BOUNDS)\n",
    "    ash_colors = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n",
    "    \n",
    "    return ash_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_mask(sample_id, split_dir):\n",
    "    masks_path = DATA_DIR + f\"/{split_dir}/{sample_id}/human_individual_masks.npy\"\n",
    "    pixel_mask = np.load(masks_path)\n",
    "    return pixel_mask\n",
    "\n",
    "def get_pixel_mask(sample_id, split_dir):\n",
    "    masks_path = DATA_DIR + f\"/{split_dir}/{sample_id}/human_pixel_masks.npy\"\n",
    "    pixel_mask = np.load(masks_path)\n",
    "    return pixel_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check `ash_colors` on one of the samples: 7829917977180135058\n",
      "(256, 256, 3)\n",
      "0.0 0.50921124\n",
      "0.097476535 0.86938816\n",
      "0.031865694 0.81146187\n"
     ]
    }
   ],
   "source": [
    "sample_id = 7829917977180135058  # train_ids[3]\n",
    "\n",
    "print(f'Check `ash_colors` on one of the samples: {sample_id}')\n",
    "\n",
    "ash_colors = get_ash_colors(sample_id, 'train')[..., N_TIMES_BEFORE]\n",
    "\n",
    "print(ash_colors.shape)\n",
    "for color in range(3):\n",
    "    array = ash_colors[..., color]\n",
    "    print(array.min(), array.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 1)\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "pixel_mask = get_pixel_mask(sample_id, 'train')\n",
    "\n",
    "print(pixel_mask.shape)\n",
    "print(pixel_mask.min(), pixel_mask.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:30:33.222846Z",
     "iopub.status.busy": "2023-07-10T19:30:33.222338Z",
     "iopub.status.idle": "2023-07-10T19:30:33.228013Z",
     "shell.execute_reply": "2023-07-10T19:30:33.227016Z",
     "shell.execute_reply.started": "2023-07-10T19:30:33.222811Z"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:  # <CONFIG>\n",
    "    \n",
    "    seed = SEED\n",
    "\n",
    "    img_size = (256, 256)\n",
    "    augmentation = True\n",
    "    \n",
    "    model = 'deeplabv3plus'  # unet | deeplabv3plus\n",
    "    preprocess = None\n",
    "    backbone_trainable = True\n",
    "    dropout = False\n",
    "    \n",
    "    num_epochs = 50  # <DEVEL> else 10\n",
    "    num_classes = 1\n",
    "    batch_size = 16  # <DEVEL> else 16 or 32\n",
    "    \n",
    "    threshold = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/keras_recipes/reproducibility_recipes/\n",
    "\n",
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) `tensorflow` random seed\n",
    "# 3) `python` random seed\n",
    "keras.utils.set_random_seed(Config.seed)\n",
    "\n",
    "# See also:\n",
    "# tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no mixed_precision\n"
     ]
    }
   ],
   "source": [
    "# Mixed Precision\n",
    "# https://www.tensorflow.org/guide/mixed_precision#supported_hardware\n",
    "\n",
    "if PLATFORM == 'kaggle':\n",
    "    MIXED_PRECISION = True\n",
    "elif PLATFORM == 'gcp':\n",
    "    # No mixed precision on QCP:\n",
    "    # \"Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0.\n",
    "    # Your GPU: Tesla P100-PCIE-16GB, compute capability 6.0\"\n",
    "    MIXED_PRECISION = False\n",
    "else:\n",
    "    raise NotImplementedError(f'unknown platform \"{PLATFORM}\"')\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    print('setting mixed_precision')\n",
    "\n",
    "    NP_FLOAT = 'float16'\n",
    "    TF_FLOAT = tf.float16\n",
    "    \n",
    "    keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    final_dtype = 'float32'\n",
    "    \n",
    "else:\n",
    "    print('no mixed_precision')\n",
    "\n",
    "    NP_FLOAT = 'float32'\n",
    "    TF_FLOAT = tf.float32\n",
    "\n",
    "    final_dtype = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet:\n",
    "    '''U-Net model.\n",
    "    \n",
    "    Inspired by and adapted from:\n",
    "    - https://keras.io/examples/vision/oxford_pets_image_segmentation\n",
    "    - https://www.kaggle.com/code/shashwatraman/simple-unet-baseline-train-lb-0-580\n",
    "    - https://www.coursera.org/learn/advanced-computer-vision-with-tensorflow/home/week/3\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, preprocess=None, weights=None):\n",
    "        self.preprocess = preprocess\n",
    "        self.weights = weights\n",
    "        \n",
    "    def conv2d_block(self, input_tensor, n_filters, kernel_size=3):\n",
    "        x = input_tensor\n",
    "        for i in range(2):\n",
    "            x = tf.keras.layers.SeparableConv2D(\n",
    "                filters = n_filters, kernel_size=(kernel_size, kernel_size), padding='same')(x)\n",
    "            #? kernel_initializer = 'he_normal'\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def encoder_block(self, inputs, n_filters, pool_size, dropout):\n",
    "        f = self.conv2d_block(inputs, n_filters=n_filters)\n",
    "        p = tf.keras.layers.MaxPooling2D(pool_size)(f)\n",
    "        p = tf.keras.layers.Dropout(dropout)(p)\n",
    "        return f, p\n",
    "\n",
    "    def encoder(self, inputs, dropout=0.1):\n",
    "        f1, p1 = self.encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=dropout)\n",
    "        f2, p2 = self.encoder_block(p1, n_filters=128, pool_size=(2,2), dropout=dropout)\n",
    "        f3, p3 = self.encoder_block(p2, n_filters=256, pool_size=(2,2), dropout=dropout)\n",
    "        f4, p4 = self.encoder_block(p3, n_filters=512, pool_size=(2,2), dropout=dropout)\n",
    "        return p4, (f1, f2, f3, f4)\n",
    "\n",
    "    def bottleneck(self, inputs):\n",
    "        bottle_neck = self.conv2d_block(inputs, n_filters=1024)\n",
    "        return bottle_neck\n",
    "\n",
    "    def decoder_block(self, inputs, conv_output, n_filters, kernel_size, strides, dropout):\n",
    "        u = tf.keras.layers.Conv2DTranspose(\n",
    "            n_filters, kernel_size, strides=strides, padding = 'same')(inputs)\n",
    "        u = tf.keras.layers.BatchNormalization()(u)\n",
    "        c = tf.keras.layers.concatenate([u, conv_output])\n",
    "        c = tf.keras.layers.Dropout(dropout)(c)\n",
    "        c = self.conv2d_block(c, n_filters, kernel_size=3)\n",
    "        return c\n",
    "\n",
    "    def decoder(self, inputs, convs, num_classes, dropout=0.1):\n",
    "        f1, f2, f3, f4 = convs\n",
    "        c6 = self.decoder_block(inputs, f4, n_filters=512, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "        c7 = self.decoder_block(c6, f3, n_filters=256, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "        c8 = self.decoder_block(c7, f2, n_filters=128, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "        c9 = self.decoder_block(c8, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "        if num_classes == 1:\n",
    "            activation = \"sigmoid\"\n",
    "        else:\n",
    "            activation = \"softmax\"\n",
    "        outputs = layers.Conv2D(\n",
    "            num_classes, kernel_size=3, activation=activation, padding=\"same\", dtype=final_dtype)(c9)\n",
    "        return outputs\n",
    "\n",
    "    def model(self, image_size, num_classes):\n",
    "        inputs = tf.keras.layers.Input(shape=(image_size,image_size,3))\n",
    "        encoder_output, convs = self.encoder(inputs)\n",
    "        #model = tf.keras.Model(inputs=inputs, outputs=encoder_output)  # debug\n",
    "        bottle_neck = self.bottleneck(encoder_output)\n",
    "        outputs = self.decoder(bottle_neck, convs, num_classes)\n",
    "        model = tf.keras.Model(name=self.__class__.__name__, inputs=inputs, outputs=outputs)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV3Plus:\n",
    "    '''DeepLabV3+ model.\n",
    "    \n",
    "    Adapted from:\n",
    "    - https://keras.io/examples/vision/deeplabv3_plus/#inference-using-colormap-overlay\n",
    "    \n",
    "    Dropout from:\n",
    "    - https://github.com/smspillaz/seg-reg\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, preprocess='resnet50', weights='imagenet', resnet50_trainable=True,\n",
    "                 dropout=False):\n",
    "        self.preprocess = preprocess\n",
    "        self.weights = weights\n",
    "        self.resnet50_trainable = resnet50_trainable\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def convolution_block(\n",
    "        self,\n",
    "        block_input,\n",
    "        num_filters=256,\n",
    "        kernel_size=3,\n",
    "        dilation_rate=1,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "    ):\n",
    "        x = layers.Conv2D(\n",
    "            num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            padding=\"same\",\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=keras.initializers.HeNormal(),\n",
    "        )(block_input)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "    def DilatedSpatialPyramidPooling(self, dspp_input):\n",
    "        dims = dspp_input.shape\n",
    "        x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "        \n",
    "        x = self.convolution_block(x, kernel_size=1, use_bias=True)\n",
    "        out_pool = layers.UpSampling2D(\n",
    "            size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "        )(x)\n",
    "\n",
    "        out_1 = self.convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "        out_6 = self.convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "        out_12 = self.convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "        out_18 = self.convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "        x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "        output = self.convolution_block(x, kernel_size=1)\n",
    "\n",
    "        if self.dropout:\n",
    "            output = tf.keras.layers.Dropout(0.1)(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def model(self, image_size, num_classes):\n",
    "        \n",
    "        model_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "        \n",
    "        resnet50 = keras.applications.ResNet50(\n",
    "            weights=self.weights, include_top=False, input_tensor=model_input,\n",
    "        )\n",
    "        resnet50.trainable = self.resnet50_trainable\n",
    "        print('resnet50.trainable =', resnet50.trainable)\n",
    "        \n",
    "        x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "        x = self.DilatedSpatialPyramidPooling(x)\n",
    "\n",
    "        input_a = layers.UpSampling2D(\n",
    "            size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
    "            interpolation=\"bilinear\",\n",
    "        )(x)\n",
    "        input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "        input_b = self.convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "        x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "        x = self.convolution_block(x)\n",
    "        if self.dropout:\n",
    "            x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        x = self.convolution_block(x)\n",
    "        if self.dropout:\n",
    "            x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = layers.UpSampling2D(\n",
    "            size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
    "            interpolation=\"bilinear\",\n",
    "        )(x)\n",
    "        \n",
    "        if num_classes == 1:\n",
    "            activation = \"sigmoid\"\n",
    "        else:\n",
    "            activation = \"softmax\"\n",
    "        model_output = layers.Conv2D(\n",
    "            num_classes, kernel_size=(1, 1), activation=activation, padding=\"same\", dtype=final_dtype)(x)\n",
    "        \n",
    "        return keras.Model(name=self.__class__.__name__, inputs=model_input, outputs=model_output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 10:10:50.999903: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-25 10:10:51.001359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-25 10:10:51.001643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-25 10:10:51.001812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-25 10:10:52.968682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-25 10:10:52.969731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-25 10:10:52.969925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-25 10:10:52.970074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15389 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet50.trainable = True\n",
      "Model: \"DeepLabV3Plus\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 262, 262, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 128, 128, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 128, 128, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 128, 128, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 130, 130, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 64, 64, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 64, 64, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 64, 64, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 64, 64, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 32, 32, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 32, 32, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 32, 32, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 16, 16, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 16, 16, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 16, 16, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 16, 16, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 256)   0           ['conv4_block6_2_relu[0][0]']    \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 1, 1, 256)    65792       ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 1, 1, 256)   1024        ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 16, 16, 256)  65536       ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 16, 16, 256)  589824      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 16, 256)  589824      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 16, 16, 256)  589824      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu (TFOpLambda)        (None, 1, 1, 256)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 16, 16, 256)  1024       ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16, 16, 256)  1024       ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 16, 16, 256)  1024       ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 16, 256)  1024       ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 16, 16, 256)  0           ['tf.nn.relu[0][0]']             \n",
      "                                                                                                  \n",
      " tf.nn.relu_1 (TFOpLambda)      (None, 16, 16, 256)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " tf.nn.relu_2 (TFOpLambda)      (None, 16, 16, 256)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " tf.nn.relu_3 (TFOpLambda)      (None, 16, 16, 256)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " tf.nn.relu_4 (TFOpLambda)      (None, 16, 16, 256)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 16, 16, 1280  0           ['up_sampling2d[0][0]',          \n",
      "                                )                                 'tf.nn.relu_1[0][0]',           \n",
      "                                                                  'tf.nn.relu_2[0][0]',           \n",
      "                                                                  'tf.nn.relu_3[0][0]',           \n",
      "                                                                  'tf.nn.relu_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 16, 16, 256)  327680      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 16, 256)  1024       ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 64, 48)   3072        ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_5 (TFOpLambda)      (None, 16, 16, 256)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64, 64, 48)  192         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0          ['tf.nn.relu_5[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.relu_6 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 304)  0           ['up_sampling2d_1[0][0]',        \n",
      "                                                                  'tf.nn.relu_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 64, 256)  700416      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.relu_7 (TFOpLambda)      (None, 64, 64, 256)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 64, 64, 256)  589824      ['tf.nn.relu_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.relu_8 (TFOpLambda)      (None, 64, 64, 256)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 25  0          ['tf.nn.relu_8[0][0]']           \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 256, 256, 1)  257         ['up_sampling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,852,353\n",
      "Trainable params: 11,819,617\n",
      "Non-trainable params: 32,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "if Config.model == 'unet':\n",
    "    builder = UNet()\n",
    "\n",
    "elif Config.model == 'deeplabv3plus':\n",
    "    builder = DeepLabV3Plus(\n",
    "        preprocess=Config.preprocess,\n",
    "        weights=resnet50_imagenet_weights,\n",
    "        resnet50_trainable=Config.backbone_trainable,\n",
    "        dropout=Config.dropout\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    raise NotImplementedError(f'model \"{Config.model}\"')\n",
    "    \n",
    "model = builder.model(image_size=Config.img_size[0], num_classes=Config.num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:30:33.222846Z",
     "iopub.status.busy": "2023-07-10T19:30:33.222338Z",
     "iopub.status.idle": "2023-07-10T19:30:33.228013Z",
     "shell.execute_reply": "2023-07-10T19:30:33.227016Z",
     "shell.execute_reply.started": "2023-07-10T19:30:33.222811Z"
    },
    "tags": []
   },
   "source": [
    "# Configure datasets (keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = None  # 512 <DEVEL> else None\n",
    "N_VALID = None  # 128 <DEVEL> else None (1856)\n",
    "N_PARTIAL = 128  # 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess = None\n"
     ]
    }
   ],
   "source": [
    "print('preprocess =', builder.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AshColorSingleFrames(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, sample_ids, split_dir, preprocess=None, n_samples=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.split_dir = split_dir\n",
    "        self.sample_ids = sample_ids[:n_samples]\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.sample_ids) / self.batch_size)\n",
    "    \n",
    "    def get_sample_ids(self, idx):\n",
    "        '''Get sample ids of batch idx.'''\n",
    "        i = idx * self.batch_size\n",
    "        return self.sample_ids[i : i + self.batch_size]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        \n",
    "        batch_sample_ids = self.get_sample_ids(idx)\n",
    "        \n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=NP_FLOAT)\n",
    "        for j, sample_id in enumerate(batch_sample_ids):\n",
    "            \n",
    "            img = get_ash_colors(sample_id, self.split_dir)[..., N_TIMES_BEFORE]\n",
    "            \n",
    "            if self.preprocess == 'resnet50':\n",
    "                img = keras.applications.resnet50.preprocess_input(img)\n",
    "            elif self.preprocess is not None:\n",
    "                raise NotImplementedError(f'preprocess \"{preprocess}\"')\n",
    "            \n",
    "            x[j] = img\n",
    "\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        if self.split_dir != 'test':\n",
    "            for j, sample_id in enumerate(batch_sample_ids):\n",
    "                img = get_pixel_mask(sample_id, self.split_dir)\n",
    "                y[j] = img\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 16 train\n",
      "number of batches: 8 valid\n",
      "number of batches: 8 partial\n",
      "number of batches: 1 test\n"
     ]
    }
   ],
   "source": [
    "train_set = AshColorSingleFrames(\n",
    "    Config.batch_size, Config.img_size, train_ids, 'train',\n",
    "    preprocess=builder.preprocess, n_samples=N_TRAIN)\n",
    "print('number of batches:', len(train_set), 'train')\n",
    "\n",
    "valid_set = AshColorSingleFrames(\n",
    "    Config.batch_size, Config.img_size, valid_ids, 'validation',\n",
    "    preprocess=builder.preprocess, n_samples=N_VALID)\n",
    "print('number of batches:', len(valid_set), 'valid')\n",
    "\n",
    "partial_set = AshColorSingleFrames(\n",
    "    Config.batch_size, Config.img_size, valid_ids, 'validation',\n",
    "    preprocess=builder.preprocess, n_samples=N_PARTIAL)\n",
    "print('number of batches:', len(partial_set), 'partial')\n",
    "\n",
    "test_set = AshColorSingleFrames(\n",
    "    Config.batch_size, Config.img_size, test_ids, 'test',\n",
    "    preprocess=builder.preprocess)\n",
    "print('number of batches:', len(test_set), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check dimensions (x, y) of first batch:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((16, 256, 256, 3), (16, 256, 256, 1))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Check dimensions (x, y) of first batch:')\n",
    "\n",
    "train_set[0][0].shape, train_set[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFDataSetCreator:\n",
    "    '''Write TFRecords files and generate a TFRecordDataset from a keras.Sequence.\n",
    "    \n",
    "    Inspired by:\n",
    "    - https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "    - https://keras.io/examples/keras_recipes/creating_tfrecords\n",
    "    - https://keras.io/examples/keras_recipes/tfrecord\n",
    "    - https://stackoverflow.com/questions/47861084/how-to-store-numpy-arrays-as-tfrecord\n",
    "    \n",
    "    References:\n",
    "    - https://www.tensorflow.org/guide/data - Build TensorFlow input pipelines\n",
    "    - https://www.tensorflow.org/guide/data_performance - Better performance with the tf.data API\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, keras_sequence, train):\n",
    "        self.keras_sequence = keras_sequence\n",
    "        self.split_dir = keras_sequence.split_dir\n",
    "        self.batch_size = keras_sequence.batch_size\n",
    "        \n",
    "        self.train = train\n",
    "        self.keep_existing = True\n",
    "        self.records_dir = os.path.join(TEMP_DIR, f'records-{self.batch_size}-{self.split_dir}')\n",
    "        self.record_paths = []\n",
    "        \n",
    "        self.progress_bar = True  # <DEVEL>\n",
    "    \n",
    "    def write_tfrec(self, batch_idx):\n",
    "        #pid = multiprocessing.current_process().pid\n",
    "        \n",
    "        record_path = os.path.join(self.records_dir, f'batch_{batch_idx:04d}.tfrec')\n",
    "        \n",
    "        if self.keep_existing and os.path.exists(record_path):\n",
    "            return record_path\n",
    "        \n",
    "        if self.progress_bar:\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('o', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "\n",
    "        x_b, y_b = self.keras_sequence[batch_idx]\n",
    "        with tf.io.TFRecordWriter(record_path) as writer:\n",
    "            for x, y in zip(x_b, y_b):\n",
    "                feature = {\n",
    "                    \"x\": tf.train.Feature(\n",
    "                        bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(x).numpy()])),\n",
    "                    \"y\": tf.train.Feature(\n",
    "                        bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(y).numpy()])),\n",
    "                }\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example.SerializeToString())\n",
    "        \n",
    "        return record_path\n",
    "    \n",
    "    def generate_tfrec(self, keep_existing=True):\n",
    "        \n",
    "        self.keep_existing = keep_existing\n",
    "        \n",
    "        records_dir = self.records_dir\n",
    "        os.makedirs(records_dir, exist_ok=True)\n",
    "        \n",
    "        n_records = len(self.keras_sequence)\n",
    "        n_procs = multiprocessing.cpu_count() * 2\n",
    "        print(f'generating {n_records} records with {n_procs} processes in: {records_dir}')\n",
    "        %ll -hd $records_dir\n",
    "\n",
    "        pool = multiprocessing.Pool(processes=n_procs)\n",
    "        batch_indexes = range(n_records)\n",
    "        record_paths = sorted(pool.map(self.write_tfrec, batch_indexes))\n",
    "        if self.progress_bar:\n",
    "            print()\n",
    "        \n",
    "        !du -sh $records_dir\n",
    "        print()\n",
    "        \n",
    "        self.record_paths = record_paths\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_tfrecord_sample(element):\n",
    "        parse_dic = {\n",
    "            'x': tf.io.FixedLenFeature([], tf.string),  # Note that it is tf.string, not tf.float32\n",
    "            'y': tf.io.FixedLenFeature([], tf.string),  # Note that it is tf.string, not tf.float32\n",
    "        }\n",
    "        feature = tf.io.parse_single_example(element, parse_dic)\n",
    "        feature['x'] = tf.io.parse_tensor(feature['x'], out_type=TF_FLOAT)\n",
    "        feature['y'] = tf.io.parse_tensor(feature['y'], out_type=tf.uint8)\n",
    "        return feature\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_sample(features):\n",
    "        return features['x'], features['y']\n",
    "\n",
    "    def randomly_crop_and_resize(self, image, label=None, scale=None):\n",
    "        \"\"\"Randomly scales image and label.\n",
    "        \n",
    "        - https://github.com/tensorflow/models/blob/.../research/deeplab/core/preprocess_utils.py\n",
    "        - https://github.com/keras-team/keras/blob/v2.13.1/keras/layers/preprocessing/image_preprocessing.py\n",
    "        \n",
    "        Args:\n",
    "        image: Image with shape [height, width, 3].\n",
    "        label: Label with shape [height, width, 1].\n",
    "        scale: The value to scale image and label. Random if None.\n",
    "\n",
    "        Returns:\n",
    "        Scaled image and label.\n",
    "        \"\"\"\n",
    "        if scale is None:\n",
    "            scale = tf.random.uniform([], minval=0.9, maxval=1.0)\n",
    "        \n",
    "        # No random scaling if scale == 1.\n",
    "        if not self.train or scale == 1.0:\n",
    "            return image, label\n",
    "        \n",
    "        image = tf.reshape(image, Config.img_size + (3,))\n",
    "        image = tf.image.central_crop(image, scale)\n",
    "        image = tf.image.resize(image, Config.img_size,\n",
    "                                method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "        if label is not None:\n",
    "            label = tf.reshape(label, Config.img_size + (1,))\n",
    "            label = tf.image.central_crop(label, scale)\n",
    "            label = tf.image.resize(label, Config.img_size,\n",
    "                                    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def dataset(self):\n",
    "        dataset = (\n",
    "            tf.data.TFRecordDataset(self.record_paths, num_parallel_reads=AUTOTUNE)\n",
    "            .map(self.parse_tfrecord_sample, num_parallel_calls=AUTOTUNE)\n",
    "            .map(self.prepare_sample, num_parallel_calls=AUTOTUNE)\n",
    "            .shuffle(self.batch_size * 10))\n",
    "        if Config.augmentation:\n",
    "            dataset = (\n",
    "                dataset\n",
    "                .map(self.randomly_crop_and_resize, num_parallel_calls=AUTOTUNE))\n",
    "        dataset = (\n",
    "            dataset\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(AUTOTUNE)\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating 16 records with 8 processes in: /home/jupyter/kaggle/temp/records-16-train\n",
      "drwxr-xr-x 2 jupyter 40K Jul 23 19:39 \u001b[0m\u001b[01;34m/home/jupyter/kaggle/temp/records-16-train\u001b[0m/\n",
      "\n",
      "17G\t/home/jupyter/kaggle/temp/records-16-train\n",
      "\n",
      "generating 8 records with 8 processes in: /home/jupyter/kaggle/temp/records-16-validation\n",
      "drwxr-xr-x 2 jupyter 4.0K Jul 23 19:41 \u001b[0m\u001b[01;34m/home/jupyter/kaggle/temp/records-16-validation\u001b[0m/\n",
      "\n",
      "1.5G\t/home/jupyter/kaggle/temp/records-16-validation\n",
      "\n",
      "generating 1 records with 8 processes in: /home/jupyter/kaggle/temp/records-16-test\n",
      "drwxr-xr-x 2 jupyter 4.0K Jul 23 18:35 \u001b[0m\u001b[01;34m/home/jupyter/kaggle/temp/records-16-test\u001b[0m/\n",
      "\n",
      "14M\t/home/jupyter/kaggle/temp/records-16-test\n",
      "\n",
      "datasets generated in: 0:00:02.122460\n"
     ]
    }
   ],
   "source": [
    "KEEP_EXISTING = True  # <DEVEL>\n",
    "\n",
    "timeit_start = datetime.datetime.now()\n",
    "\n",
    "#tf.config.run_functions_eagerly(False)\n",
    "#tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Place tensors on the CPU\n",
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    tf_train_set = (\n",
    "        TFDataSetCreator(train_set, train=True)\n",
    "        .generate_tfrec(KEEP_EXISTING)\n",
    "        .dataset()\n",
    "    )\n",
    "\n",
    "    tf_valid_set = (\n",
    "        TFDataSetCreator(valid_set, train=False)\n",
    "        .generate_tfrec(KEEP_EXISTING)\n",
    "        .dataset()\n",
    "    )\n",
    "\n",
    "    tf_test_set = (\n",
    "        TFDataSetCreator(test_set, train=False)\n",
    "        .generate_tfrec(KEEP_EXISTING)\n",
    "        .dataset()\n",
    "    )\n",
    "\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "timeit_end = datetime.datetime.now()\n",
    "print('datasets generated in:', timeit_end - timeit_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=0.001, threshold=None):\n",
    "    '''Dice coefficient.\n",
    "    \n",
    "    Adapted from:\n",
    "    - https://stackoverflow.com/questions/72195156/correct-implementation-of-dice-loss-in-tensorflow-keras\n",
    "    - https://www.kaggle.com/code/shashwatraman/simple-unet-baseline-train-lb-0-580\n",
    "    '''\n",
    "    \n",
    "    y_true_f = backend.flatten(tf.cast(y_true, TF_FLOAT))\n",
    "    y_pred_f = backend.flatten(tf.cast(y_pred, TF_FLOAT))\n",
    "    # ValueError: No gradients provided for any variable\n",
    "    if threshold is not None:\n",
    "        y_pred_f = backend.flatten(\n",
    "            tf.cast(tf.math.greater(tf.cast(y_pred, TF_FLOAT), threshold), TF_FLOAT))\n",
    "    intersection = backend.sum(y_true_f * y_pred_f)\n",
    "    dice = (2. * intersection + smooth) / (backend.sum(y_true_f) + backend.sum(y_pred_f) + smooth)\n",
    "    return dice\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def threshold_dice_coef(y_true, y_pred, smooth=0.001):\n",
    "#    '''Dice coefficient with threshold set to Config.threshold.'''\n",
    "#    return dice_coef(y_true, y_pred, smooth=smooth, threshold=Config.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check dice_coef() on one of the samples: 7829917977180135058\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.8743467, shape=(), dtype=float32)\n",
      "tf.Tensor(0.83587146, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7393573, shape=(), dtype=float32)\n",
      "tf.Tensor(0.8522139, shape=(), dtype=float32)\n",
      "tf.Tensor(0.87988245, shape=(), dtype=float32)\n",
      "tf.Tensor(0.84089667, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample_id = 7829917977180135058  # train_ids[3]\n",
    "\n",
    "print(f'Check dice_coef() on one of the samples: {sample_id}')\n",
    "\n",
    "merged_mask = get_pixel_mask(sample_id, 'train')\n",
    "indiv_masks = get_individual_mask(sample_id, 'train')\n",
    "\n",
    "print(dice_coef(tf.convert_to_tensor(merged_mask),\n",
    "                tf.convert_to_tensor(merged_mask)))\n",
    "for idv in range(6):\n",
    "    print(dice_coef(tf.convert_to_tensor(merged_mask),\n",
    "                    tf.convert_to_tensor(indiv_masks[..., idv])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint file: identify-contrails_2023-07-25_12-10-42.h5\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = f'identify-contrails_{file_time_str}.h5'\n",
    "logger_path = f'identify-contrails_{file_time_str}_log.csv'\n",
    "\n",
    "print(f'checkpoint file: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler:\n",
    "# - https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules\n",
    "# - https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay\n",
    "# Note: CosineDecay got warmup from v2.13.1 on\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1  # per epoch\n",
    "decay_rate = 0.9\n",
    "\n",
    "cos_scheduler = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate, decay_steps)\n",
    "\n",
    "exp_scheduler = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the model for training.\n",
    "\n",
    "# We use the \"sparse\" version of categorical_crossentropy\n",
    "# because our target data is integers.\n",
    "# See also:\n",
    "# loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[dice_coef])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.LearningRateScheduler(exp_scheduler),\n",
    "    keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True),\n",
    "    keras.callbacks.CSVLogger(logger_path, append=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LET'S TRAIN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 10:11:20.925990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n",
      "2023-07-25 10:11:25.599896: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x562b830bb130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-25 10:11:25.599935: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2023-07-25 10:11:25.647365: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-25 10:11:26.094570: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 45s 455ms/step - loss: 0.1915 - dice_coef: 0.0127 - val_loss: 15.2182 - val_dice_coef: 0.0040 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 6s 379ms/step - loss: 0.0355 - dice_coef: 0.0224 - val_loss: 0.5368 - val_dice_coef: 0.0042 - lr: 9.0000e-04\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 6s 369ms/step - loss: 0.0298 - dice_coef: 0.0310 - val_loss: 0.0336 - val_dice_coef: 0.0037 - lr: 8.1000e-04\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0263 - dice_coef: 0.0701 - val_loss: 0.0387 - val_dice_coef: 0.0038 - lr: 7.2900e-04\n",
      "Epoch 5/50\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0226 - dice_coef: 0.0897"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel loaded weights from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprev_checkpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Train the model, doing validation at the end of each epoch.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf_train_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_valid_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if LOAD_CHECKPOINT:\n",
    "    # Loads the weights\n",
    "    model.load_weights(prev_checkpoint_path)\n",
    "    print(f'model loaded weights from {prev_checkpoint_path}')\n",
    "\n",
    "if TRAIN:\n",
    "    # Train the model, doing validation at the end of each epoch.\n",
    "    history = model.fit(\n",
    "        tf_train_set, epochs=Config.num_epochs, validation_data=tf_valid_set, callbacks=callbacks,\n",
    "        workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    print('History', history.history.keys())\n",
    "\n",
    "    for var, yrange in [('loss', [0.0, 0.02]),\n",
    "                        ('dice_coef', [0.0, 0.8])]:\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(history.history[var])\n",
    "        plt.plot(history.history[f'val_{var}'])\n",
    "        plt.ylim(yrange[0], yrange[1])\n",
    "        plt.title(f'model {var}')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel(var)\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(pred, threshold):\n",
    "    return (pred > threshold).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATE = True\n",
    "ALL_BATCHES = True\n",
    "BATCH_IDX = 0\n",
    "SAMPLE_IDX = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:    \n",
    "    # Evaluate the model\n",
    "\n",
    "    if ALL_BATCHES:\n",
    "        loss, acc = model.evaluate(partial_set, verbose=2)\n",
    "    else:\n",
    "        eval_images, eval_masks = partial_set[BATCH_IDX]\n",
    "        loss, acc = model.evaluate(eval_images, eval_masks, verbose=2)\n",
    "\n",
    "    print(\"Model accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dice_coef(sample_set, pred_set, batch_size, threshold):\n",
    "    dice_coef_per_batch = np.full(len(sample_set), np.nan)\n",
    "    for idx in range(len(sample_set)):\n",
    "        x, y = sample_set[idx]\n",
    "        pred = pred_set[idx*batch_size:(idx + 1)*batch_size]\n",
    "        _coef = dice_coef(y, pred, threshold=threshold)\n",
    "        dice_coef_per_batch[idx] = _coef\n",
    "    return dice_coef_per_batch\n",
    "\n",
    "if EVALUATE:\n",
    "    \n",
    "    predictions = model.predict(partial_set)\n",
    "    \n",
    "    _coefs = eval_dice_coef(\n",
    "        partial_set, predictions, batch_size=partial_set.batch_size, threshold=None)\n",
    "    print(f'w/o threshold: {_coefs.mean():.2%}')\n",
    "    \n",
    "    if Config.threshold == 'auto':\n",
    "        best_coef = 0.\n",
    "        for threshold in np.arange(0.1, 0.8, 0.1):\n",
    "            _coefs = eval_dice_coef(\n",
    "                partial_set, predictions, batch_size=partial_set.batch_size, threshold=threshold)\n",
    "            mean_coef = _coefs.mean()\n",
    "            if mean_coef > best_coef:\n",
    "                best_coef = mean_coef\n",
    "                best_thresh = threshold\n",
    "            print(f'{threshold:.02} threshold: {mean_coef:.2%}')\n",
    "        print(f'best_threshold = {best_thresh:.02}')\n",
    "        Config.threshold = best_thresh\n",
    "        print('Config.threshold updated')\n",
    "    else:\n",
    "        threshold = Config.threshold\n",
    "        _coefs = eval_dice_coef(\n",
    "            partial_set, predictions, batch_size=partial_set.batch_size, threshold=threshold)\n",
    "        mean_coef = _coefs.mean()\n",
    "        print(f'{threshold:.02} threshold: {mean_coef:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(img, truth, pred):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 8))\n",
    "\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title(\"Ash Color Image\")\n",
    "\n",
    "    axs[1].imshow(truth)\n",
    "    axs[1].set_title(\"Ground Truth\")\n",
    "\n",
    "    axs[2].imshow(pred)\n",
    "    axs[2].set_title(\"Prediction\")\n",
    "\n",
    "    axs[3].imshow(img)\n",
    "    axs[3].imshow(truth, cmap='Reds', alpha=.3, interpolation='none')\n",
    "    axs[3].set_title('Contrail mask on ash color image')\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "if EVALUATE:\n",
    "    eval_images, eval_masks = partial_set[BATCH_IDX]\n",
    "    idx = SAMPLE_IDX\n",
    "    threshold = Config.threshold\n",
    "    plot_prediction(\n",
    "        eval_images[idx], eval_masks[idx], apply_threshold(predictions[idx], threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(x, fg_val=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x:  numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns: run length encoding as list\n",
    "    \"\"\"\n",
    "\n",
    "    dots = np.where(\n",
    "        x.T.flatten() == fg_val)[0]  # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def list_to_string(x):\n",
    "    \"\"\"\n",
    "    Converts list to a string representation\n",
    "    Empty list returns '-'\n",
    "    \"\"\"\n",
    "    if x: # non-empty list\n",
    "        s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")\n",
    "    else:\n",
    "        s = '-'\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recs = os.listdir(os.path.join(DATA_DIR, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'), index_col='record_id')[0:0]\n",
    "\n",
    "for test_id, pred in zip(test_ids, predictions):\n",
    "    \n",
    "    mask = apply_threshold(pred, Config.threshold)\n",
    "    \n",
    "    # notice the we're converting rec to an `int` here:\n",
    "    submission.loc[int(test_id), 'encoded_pixels'] = list_to_string(rle_encode(mask))\n",
    "    \n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strf_timedelta(timedelta):\n",
    "    total_seconds = timedelta.total_seconds()\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return '{:02}:{:02}:{:02}'.format(int(hours), int(minutes), int(seconds))\n",
    "\n",
    "end_time = datetime.datetime.now(timezone('CET'))\n",
    "\n",
    "print('Terminated', end_time.strftime(PRINT_TIME_FORMAT),\n",
    "      'in', strf_timedelta(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS THE END!"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
