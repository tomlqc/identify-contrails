{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Contrails with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reinstall tensorflow-io\n",
    "# to avoid the UserWarning: unable to load libtensorflow_io_plugins.so\n",
    "\n",
    "#!pip install tensorflow-io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "\n",
    "TRAIN = True\n",
    "#PLATFORM = 'kaggle'\n",
    "PLATFORM = 'gcp'\n",
    "\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%cp /kaggle/input/identify-contrails/contrails_2023-07-15_15-10-49.h5 .\n",
    "#%mv weights_v8_contrails-unet_cst-lr.h5 contrails-unet_cst-lr.h5\n",
    "\n",
    "if not TRAIN:\n",
    "    checkpoint_path = 'contrails_2023-07-15_15-10-49.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from pytz import timezone\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRINT_TIME_FORMAT = \"%Y-%m-%d %H:%M:%S %Z%z\"\n",
    "FILE_TIME_FORMAT = \"%Y-%m-%d_%H-%M-%S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 2023-07-18 23:33:59 CEST+0200\n"
     ]
    }
   ],
   "source": [
    "now_time = datetime.datetime.now(timezone('CET'))\n",
    "\n",
    "file_time_str = now_time.strftime(FILE_TIME_FORMAT)\n",
    "\n",
    "print('Started', now_time.strftime(PRINT_TIME_FORMAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num CPUs Available:  1\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/kaggle/working'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following U-Net model is adapted from:\n",
    "- https://keras.io/examples/vision/oxford_pets_image_segmentation\n",
    "- https://www.kaggle.com/code/shashwatraman/simple-unet-baseline-train-lb-0-580"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "if PLATFORM == 'kaggle':\n",
    "\n",
    "    WORK_DIR = '/kaggle/working'  # preserved if notebook is saved\n",
    "    TEMP_DIR = '/kaggle/temp'  # just during current session\n",
    "\n",
    "    DATA_DIR = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n",
    "\n",
    "elif PLATFORM == 'gcp':\n",
    "\n",
    "    WORK_DIR = '/home/jupyter/kaggle/working'  # preserved if notebook is saved\n",
    "    TEMP_DIR = '/home/jupyter/kaggle/temp'  # just during current session\n",
    "\n",
    "    DATA_DIR = '/home/jupyter/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n",
    "    \n",
    "    %cd $WORK_DIR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Paths:\n",
    "    train = os.path.join(DATA_DIR, 'train')\n",
    "    valid = os.path.join(DATA_DIR, 'validation')\n",
    "    test = os.path.join(DATA_DIR, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20529 1856 2\n"
     ]
    }
   ],
   "source": [
    "train_ids = sorted(os.listdir(Paths.train))\n",
    "valid_ids = sorted(os.listdir(Paths.valid))\n",
    "test_ids = sorted(os.listdir(Paths.test))\n",
    "print(len(train_ids), len(valid_ids), len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ABI:\n",
    "    bands = {name: idx for idx, name in enumerate([\n",
    "        '08', '09', '10', '11', '12', '13', '14', '15', '16'])}\n",
    "    colors = {name: idx for idx, name in enumerate([\n",
    "        'red', 'blue', 'green', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_TIMES_BEFORE = 4\n",
    "N_TIMES_AFTER = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_range(data, bounds):\n",
    "    \"\"\"Maps data to the range [0, 1].\"\"\"\n",
    "    return (data - bounds[0]) / (bounds[1] - bounds[0])\n",
    "\n",
    "_T11_BOUNDS = (243, 303)\n",
    "_CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n",
    "_TDIFF_BOUNDS = (-4, 2)\n",
    "\n",
    "def get_ash_colors(sample_id, split_dir):\n",
    "    \"\"\"\n",
    "    Based on bands: 11, 14, 15\n",
    "    \n",
    "    Args:\n",
    "        sample_id(str): The id of the example i.e. '1000216489776414077'\n",
    "        split_dir(str): The split directoryu i.e. 'test', 'train', 'val'\n",
    "    \"\"\"\n",
    "    band15 = np.load(DATA_DIR + f\"/{split_dir}/{sample_id}/band_15.npy\")\n",
    "    band14 = np.load(DATA_DIR + f\"/{split_dir}/{sample_id}/band_14.npy\")\n",
    "    band11 = np.load(DATA_DIR + f\"/{split_dir}/{sample_id}/band_11.npy\")\n",
    "\n",
    "    r = normalize_range(band15 - band14, _TDIFF_BOUNDS)\n",
    "    g = normalize_range(band14 - band11, _CLOUD_TOP_TDIFF_BOUNDS)\n",
    "    b = normalize_range(band14, _T11_BOUNDS)\n",
    "    ash_colors = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n",
    "    \n",
    "    return ash_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_individual_mask(sample_id, split_dir):\n",
    "    masks_path = DATA_DIR + f\"/{split_dir}/{sample_id}/human_individual_masks.npy\"\n",
    "    pixel_mask = np.load(masks_path)\n",
    "    return pixel_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pixel_mask(sample_id, split_dir):\n",
    "    masks_path = DATA_DIR + f\"/{split_dir}/{sample_id}/human_pixel_masks.npy\"\n",
    "    pixel_mask = np.load(masks_path)\n",
    "    return pixel_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some values (DEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "0.0 0.6714986\n",
      "0.0 0.8929291\n",
      "0.0 0.8980291\n"
     ]
    }
   ],
   "source": [
    "sample_id = train_ids[3]\n",
    "\n",
    "ash_colors = get_ash_colors(sample_id, 'train')[..., N_TIMES_BEFORE]\n",
    "\n",
    "print(ash_colors.shape)\n",
    "for color in range(3):\n",
    "    array = ash_colors[..., color]\n",
    "    print(array.min(), array.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 1)\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "pixel_mask = get_pixel_mask(sample_id, 'train')\n",
    "\n",
    "print(pixel_mask.shape)\n",
    "print(pixel_mask.min(), pixel_mask.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:30:33.222846Z",
     "iopub.status.busy": "2023-07-10T19:30:33.222338Z",
     "iopub.status.idle": "2023-07-10T19:30:33.228013Z",
     "shell.execute_reply": "2023-07-10T19:30:33.227016Z",
     "shell.execute_reply.started": "2023-07-10T19:30:33.222811Z"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    img_size = (256, 256)\n",
    "    \n",
    "    num_epochs = 10  # <DEVEL> else 10\n",
    "    num_classes = 1\n",
    "    batch_size = 16  # <DEVEL> else 32\n",
    "    \n",
    "    threshold = 0.40\n",
    "    \n",
    "    seed = SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://keras.io/examples/keras_recipes/reproducibility_recipes/\n",
    "\n",
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) `tensorflow` random seed\n",
    "# 3) `python` random seed\n",
    "keras.utils.set_random_seed(Config.seed)\n",
    "\n",
    "# See also:\n",
    "# tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model inspired by:\n",
    "- https://www.coursera.org/learn/advanced-computer-vision-with-tensorflow/\n",
    "- file:///D:/Courses/2023-07_Advanced-Computer-Vision-with-TensorFlow/lecture-notes/C3_W3_Image-Segmentation.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
    "    x = input_tensor\n",
    "    for i in range(2):\n",
    "        x = tf.keras.layers.SeparableConv2D(\n",
    "            filters = n_filters, kernel_size=(kernel_size, kernel_size), padding='same')(x)\n",
    "        #? kernel_initializer = 'he_normal'\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encoder_block(inputs, n_filters, pool_size, dropout):\n",
    "    f = conv2d_block(inputs, n_filters=n_filters)\n",
    "    p = tf.keras.layers.MaxPooling2D(pool_size)(f)\n",
    "    p = tf.keras.layers.Dropout(dropout)(p)\n",
    "    return f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encoder(inputs, dropout=0.1):\n",
    "    f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=dropout)\n",
    "    f2, p2 = encoder_block(p1, n_filters=128, pool_size=(2,2), dropout=dropout)\n",
    "    f3, p3 = encoder_block(p2, n_filters=256, pool_size=(2,2), dropout=dropout)\n",
    "    f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2,2), dropout=dropout)\n",
    "    return p4, (f1, f2, f3, f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bottleneck(inputs):\n",
    "    bottle_neck = conv2d_block(inputs, n_filters=1024)\n",
    "    return bottle_neck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decoder_block(inputs, conv_output, n_filters, kernel_size, strides, dropout):\n",
    "    u = tf.keras.layers.Conv2DTranspose(\n",
    "        n_filters, kernel_size, strides=strides, padding = 'same')(inputs)\n",
    "    u = tf.keras.layers.BatchNormalization()(u)\n",
    "    c = tf.keras.layers.concatenate([u, conv_output])\n",
    "    c = tf.keras.layers.Dropout(dropout)(c)\n",
    "    c = conv2d_block(c, n_filters, kernel_size=3)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decoder(inputs, convs, num_classes, dropout=0.1):\n",
    "    f1, f2, f3, f4 = convs\n",
    "    c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "    c7 = decoder_block(c6, f3, n_filters=256, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "    c8 = decoder_block(c7, f2, n_filters=128, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "    c9 = decoder_block(c8, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "    if num_classes == 1:\n",
    "        activation = \"sigmoid\"\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "    outputs = layers.Conv2D(num_classes, kernel_size=3, activation=activation, padding=\"same\")(c9)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unet(image_size, num_classes):\n",
    "    inputs = tf.keras.layers.Input(shape=(image_size,image_size,3))\n",
    "    encoder_output, convs = encoder(inputs)\n",
    "    #model = tf.keras.Model(inputs=inputs, outputs=encoder_output)  # debug\n",
    "    bottle_neck = bottleneck(encoder_output)\n",
    "    outputs = decoder(bottle_neck, convs, num_classes)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 256, 256, 64  283        ['input_1[0][0]']                \n",
      " v2D)                           )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256, 256, 64  256        ['separable_conv2d[0][0]']       \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 256, 256, 64  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 256, 256, 64  4736       ['activation[0][0]']             \n",
      " onv2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 256, 256, 64  256        ['separable_conv2d_1[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 256, 256, 64  0           ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 128, 128, 64  0           ['activation_1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128, 128, 64  0           ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 128, 128, 12  8896       ['dropout[0][0]']                \n",
      " onv2D)                         8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128, 128, 12  512        ['separable_conv2d_2[0][0]']     \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_2[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 128, 128, 12  17664      ['activation_2[0][0]']           \n",
      " onv2D)                         8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128, 128, 12  512        ['separable_conv2d_3[0][0]']     \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_3[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0          ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64, 64, 128)  0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 64, 64, 256)  34176      ['dropout_1[0][0]']              \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 64, 256)  1024       ['separable_conv2d_4[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 64, 64, 256)  68096      ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64, 64, 256)  1024       ['separable_conv2d_5[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0          ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32, 32, 256)  0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " separable_conv2d_6 (SeparableC  (None, 32, 32, 512)  133888     ['dropout_2[0][0]']              \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 512)  2048       ['separable_conv2d_6[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 512)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_7 (SeparableC  (None, 32, 32, 512)  267264     ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 512)  2048       ['separable_conv2d_7[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 512)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0          ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 16, 16, 512)  0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " separable_conv2d_8 (SeparableC  (None, 16, 16, 1024  529920     ['dropout_3[0][0]']              \n",
      " onv2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 1024  4096       ['separable_conv2d_8[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 1024  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_9 (SeparableC  (None, 16, 16, 1024  1058816    ['activation_8[0][0]']           \n",
      " onv2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 1024  4096       ['separable_conv2d_9[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 1024  0           ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 32, 32, 512)  4719104    ['activation_9[0][0]']           \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_transpose[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 1024  0           ['batch_normalization_10[0][0]', \n",
      "                                )                                 'activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 32, 32, 1024  0           ['concatenate[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_10 (Separable  (None, 32, 32, 512)  534016     ['dropout_4[0][0]']              \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 512)  2048       ['separable_conv2d_10[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_11 (Separable  (None, 32, 32, 512)  267264     ['activation_10[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 512)  2048       ['separable_conv2d_11[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 256)  1179904    ['activation_11[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_transpose_1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 512)  0           ['batch_normalization_13[0][0]', \n",
      "                                                                  'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64, 64, 512)  0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " separable_conv2d_12 (Separable  (None, 64, 64, 256)  135936     ['dropout_5[0][0]']              \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 64, 64, 256)  1024       ['separable_conv2d_12[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_13 (Separable  (None, 64, 64, 256)  68096      ['activation_12[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 64, 64, 256)  1024       ['separable_conv2d_13[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 12  295040     ['activation_13[0][0]']          \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 128, 128, 12  512        ['conv2d_transpose_2[0][0]']     \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 128, 25  0           ['batch_normalization_16[0][0]', \n",
      "                                6)                                'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 128, 128, 25  0           ['concatenate_2[0][0]']          \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_14 (Separable  (None, 128, 128, 12  35200      ['dropout_6[0][0]']              \n",
      " Conv2D)                        8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 128, 128, 12  512        ['separable_conv2d_14[0][0]']    \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_17[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_15 (Separable  (None, 128, 128, 12  17664      ['activation_14[0][0]']          \n",
      " Conv2D)                        8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 128, 128, 12  512        ['separable_conv2d_15[0][0]']    \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_18[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 256, 256, 64  73792      ['activation_15[0][0]']          \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 256, 256, 64  256        ['conv2d_transpose_3[0][0]']     \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 256, 256, 12  0           ['batch_normalization_19[0][0]', \n",
      "                                8)                                'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 256, 256, 12  0           ['concatenate_3[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_16 (Separable  (None, 256, 256, 64  9408       ['dropout_7[0][0]']              \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 256, 256, 64  256        ['separable_conv2d_16[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_20[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_17 (Separable  (None, 256, 256, 64  4736       ['activation_16[0][0]']          \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 256, 256, 64  256        ['separable_conv2d_17[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_21[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 256, 256, 1)  577         ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,491,868\n",
      "Trainable params: 9,478,172\n",
      "Non-trainable params: 13,696\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "#model = get_model(Config.img_size, Config.num_classes)\n",
    "model = unet(image_size=256, num_classes=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:30:33.222846Z",
     "iopub.status.busy": "2023-07-10T19:30:33.222338Z",
     "iopub.status.idle": "2023-07-10T19:30:33.228013Z",
     "shell.execute_reply": "2023-07-10T19:30:33.227016Z",
     "shell.execute_reply.started": "2023-07-10T19:30:33.222811Z"
    }
   },
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = None  # None to take all\n",
    "N_PARTIAL = 128  # 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AshColorSingleFrames(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, sample_ids, split_dir, n_samples=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.split_dir = split_dir\n",
    "        self.sample_ids = sample_ids[:n_samples]\n",
    "        self.cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.sample_ids) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        print(f'getting {idx}')\n",
    "        if idx not in self.cache:\n",
    "            self.cache.clear()\n",
    "            # Load several batches in advance.\n",
    "            for i in range(idx, min(idx + 10, len(self))):\n",
    "                self.cache[i] = self.__load(i)\n",
    "        return self.cache[idx]\n",
    "#        return self.__load(idx)\n",
    "            \n",
    "    def __load(self, idx):\n",
    "        print(f'loading {idx}')\n",
    "        \n",
    "        i = idx * self.batch_size\n",
    "        batch_sample_ids = self.sample_ids[i : i + self.batch_size]\n",
    "        \n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, sample_id in enumerate(batch_sample_ids):\n",
    "            img = get_ash_colors(sample_id, self.split_dir)\n",
    "            x[j] = img[..., N_TIMES_BEFORE]\n",
    "\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        if self.split_dir != 'test':\n",
    "            for j, sample_id in enumerate(batch_sample_ids):\n",
    "                img = get_pixel_mask(sample_id, self.split_dir)\n",
    "                y[j] = img\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 1284\n"
     ]
    }
   ],
   "source": [
    "train_set = AshColorSingleFrames(Config.batch_size, Config.img_size, train_ids, 'train',\n",
    "                                 n_samples=N_SAMPLES)  # <DEVEL>\n",
    "print('number of batches:', len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 116\n"
     ]
    }
   ],
   "source": [
    "valid_set = AshColorSingleFrames(Config.batch_size, Config.img_size, valid_ids, 'validation',\n",
    "                                 n_samples=N_SAMPLES)  # <DEVEL>\n",
    "print('number of batches:', len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 8\n"
     ]
    }
   ],
   "source": [
    "partial_set = AshColorSingleFrames(Config.batch_size, Config.img_size, valid_ids, 'validation',\n",
    "                                 n_samples=N_PARTIAL)  # <DEVEL>\n",
    "print('number of batches:', len(partial_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 1\n"
     ]
    }
   ],
   "source": [
    "test_set = AshColorSingleFrames(Config.batch_size, Config.img_size, test_ids, 'test')\n",
    "print('number of batches:', len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check batch dimensions (x, y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting 0\n",
      "loading 0\n",
      "loading 1\n",
      "loading 2\n",
      "loading 3\n",
      "loading 4\n",
      "loading 5\n",
      "loading 6\n",
      "loading 7\n",
      "loading 8\n",
      "loading 9\n",
      "getting 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((16, 256, 256, 3), (16, 256, 256, 1))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0].shape, train_set[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting 14\n",
      "getting 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((16, 256, 256, 3), (16, 256, 256, 1))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[14][0].shape, train_set[14][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dice_coef` adapted from:\n",
    "- https://stackoverflow.com/questions/72195156/correct-implementation-of-dice-loss-in-tensorflow-keras\n",
    "- https://www.kaggle.com/code/shashwatraman/simple-unet-baseline-train-lb-0-580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=0.001, threshold=None):\n",
    "    y_true_f = backend.flatten(tf.cast(y_true, tf.float32))\n",
    "    y_pred_f = backend.flatten(tf.cast(y_pred, tf.float32))\n",
    "    # ValueError: No gradients provided for any variable\n",
    "    if threshold is not None:\n",
    "        y_pred_f = backend.flatten(\n",
    "            tf.cast(tf.math.greater(tf.cast(y_pred, tf.float32), threshold), tf.float32))\n",
    "    intersection = backend.sum(y_true_f * y_pred_f)\n",
    "    dice = (2. * intersection + smooth) / (backend.sum(y_true_f) + backend.sum(y_pred_f) + smooth)\n",
    "    return dice\n",
    "\n",
    "def threshold_dice_coef(y_true, y_pred, smooth=0.001):\n",
    "    return dice_coef(y_true, y_pred, smooth=smooth, threshold=Config.threshold)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check `dice_coef()` on one of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_id: 100071707854144929\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample_id = train_ids[3]\n",
    "print(f'sample_id: {sample_id}')\n",
    "\n",
    "merged_mask = get_pixel_mask(sample_id, 'train')\n",
    "indiv_masks = get_individual_mask(sample_id, 'train')\n",
    "\n",
    "print(dice_coef(tf.convert_to_tensor(merged_mask),\n",
    "                tf.convert_to_tensor(merged_mask)))\n",
    "for idv in range(6):\n",
    "    print(dice_coef(tf.convert_to_tensor(merged_mask),\n",
    "                    tf.convert_to_tensor(indiv_masks[..., idv])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate scheduler:\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay - warmup only from v2.13.1 on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "decay_steps = len(train_set)\n",
    "decay_rate = 0.7\n",
    "\n",
    "cos_scheduler = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate, decay_steps)\n",
    "\n",
    "exp_scheduler = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint file: contrails_2023-07-18_23-33-59.h5\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    checkpoint_path = f\"contrails_{file_time_str}.h5\"\n",
    "\n",
    "print(f'checkpoint file: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the model for training.\n",
    "# We use the \"sparse\" version of categorical_crossentropy\n",
    "# because our target data is integers.\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[dice_coef])\n",
    "#model.compile(optimizer=\"adam\", loss=dice_loss, metrics=[dice_coef])\n",
    "\n",
    "callbacks = [\n",
    "    #keras.callbacks.LearningRateScheduler(exp_scheduler),\n",
    "    keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting 0\n",
      "loading 0\n",
      "loading 1\n",
      "loading 2\n",
      "loading 3\n",
      "loading 4\n",
      "loading 5\n",
      "loading 6\n",
      "loading 7\n",
      "loading 8\n",
      "loading 9\n",
      "Epoch 1/10\n",
      "getting 0\n",
      "getting 1\n",
      "   1/1284 [..............................] - ETA: 22:13 - loss: 0.0869 - dice_coef: 0.0193getting 2\n",
      "   2/1284 [..............................] - ETA: 21:40 - loss: 0.0742 - dice_coef: 0.0129getting 3\n",
      "   3/1284 [..............................] - ETA: 21:39 - loss: 0.0658 - dice_coef: 0.0101getting 4\n",
      "   4/1284 [..............................] - ETA: 21:38 - loss: 0.0682 - dice_coef: 0.0119getting 5\n",
      "   5/1284 [..............................] - ETA: 21:38 - loss: 0.0656 - dice_coef: 0.0120getting 6\n",
      "   6/1284 [..............................] - ETA: 21:37 - loss: 0.0637 - dice_coef: 0.0120getting 7\n",
      "   7/1284 [..............................] - ETA: 21:36 - loss: 0.0624 - dice_coef: 0.0120getting 8\n",
      "   8/1284 [..............................] - ETA: 21:35 - loss: 0.0597 - dice_coef: 0.0115getting 9\n",
      "   9/1284 [..............................] - ETA: 21:34 - loss: 0.0605 - dice_coef: 0.0120getting 10\n",
      "loading 10\n",
      "  10/1284 [..............................] - ETA: 21:33 - loss: 0.0584 - dice_coef: 0.0115loading 11\n",
      "loading 12\n",
      "loading 13\n",
      "loading 14\n",
      "loading 15\n",
      "loading 16\n",
      "loading 17\n",
      "loading 18\n",
      "loading 19\n",
      "getting 11\n",
      "  11/1284 [..............................] - ETA: 41:21 - loss: 0.0558 - dice_coef: 0.0110getting 12\n",
      "  12/1284 [..............................] - ETA: 39:31 - loss: 0.0539 - dice_coef: 0.0106getting 13\n",
      "  13/1284 [..............................] - ETA: 38:00 - loss: 0.0520 - dice_coef: 0.0102getting 14\n",
      "  14/1284 [..............................] - ETA: 36:42 - loss: 0.0502 - dice_coef: 0.0097getting 15\n",
      "  15/1284 [..............................] - ETA: 35:35 - loss: 0.0496 - dice_coef: 0.0097getting 16\n",
      "  16/1284 [..............................] - ETA: 34:37 - loss: 0.0481 - dice_coef: 0.0094getting 17\n",
      "  17/1284 [..............................] - ETA: 33:46 - loss: 0.0474 - dice_coef: 0.0093getting 18\n",
      "  18/1284 [..............................] - ETA: 33:01 - loss: 0.0467 - dice_coef: 0.0093getting 19\n",
      "  19/1284 [..............................] - ETA: 32:21 - loss: 0.0453 - dice_coef: 0.0089getting 20\n",
      "loading 20\n",
      "loading 21\n",
      "loading 22\n",
      "  20/1284 [..............................] - ETA: 31:45 - loss: 0.0445 - dice_coef: 0.0088loading 23\n",
      "loading 24\n",
      "loading 25\n",
      "loading 26\n",
      "loading 27\n",
      "loading 28\n",
      "loading 29\n",
      "getting 21\n",
      "  21/1284 [..............................] - ETA: 1:09:10 - loss: 0.0434 - dice_coef: 0.0086getting 22\n",
      "  22/1284 [..............................] - ETA: 1:06:51 - loss: 0.0445 - dice_coef: 0.0088getting 23\n",
      "  23/1284 [..............................] - ETA: 1:04:44 - loss: 0.0436 - dice_coef: 0.0087getting 24\n",
      "  24/1284 [..............................] - ETA: 1:02:48 - loss: 0.0427 - dice_coef: 0.0085getting 25\n",
      "  25/1284 [..............................] - ETA: 1:01:01 - loss: 0.0426 - dice_coef: 0.0086getting 26\n",
      "  26/1284 [..............................] - ETA: 59:23 - loss: 0.0428 - dice_coef: 0.0086  getting 27\n",
      "  27/1284 [..............................] - ETA: 57:52 - loss: 0.0429 - dice_coef: 0.0087getting 28\n",
      "  28/1284 [..............................] - ETA: 56:28 - loss: 0.0426 - dice_coef: 0.0086getting 29\n",
      "  29/1284 [..............................] - ETA: 55:10 - loss: 0.0417 - dice_coef: 0.0085getting 30\n",
      "loading 30\n",
      "  30/1284 [..............................] - ETA: 53:57 - loss: 0.0410 - dice_coef: 0.0083loading 31\n",
      "loading 32\n",
      "loading 33\n",
      "loading 34\n",
      "loading 35\n",
      "loading 36\n",
      "loading 37\n",
      "loading 38\n",
      "loading 39\n",
      "getting 31\n",
      "  31/1284 [..............................] - ETA: 1:22:20 - loss: 0.0405 - dice_coef: 0.0082getting 32\n",
      "  32/1284 [..............................] - ETA: 1:20:17 - loss: 0.0402 - dice_coef: 0.0082getting 33\n",
      "  33/1284 [..............................] - ETA: 1:18:23 - loss: 0.0399 - dice_coef: 0.0081getting 34\n",
      "  34/1284 [..............................] - ETA: 1:16:35 - loss: 0.0394 - dice_coef: 0.0081getting 35\n",
      "  35/1284 [..............................] - ETA: 1:14:54 - loss: 0.0393 - dice_coef: 0.0081getting 36\n",
      "  36/1284 [..............................] - ETA: 1:13:18 - loss: 0.0398 - dice_coef: 0.0081getting 37\n",
      "  37/1284 [..............................] - ETA: 1:11:48 - loss: 0.0395 - dice_coef: 0.0080getting 38\n",
      "  38/1284 [..............................] - ETA: 1:10:22 - loss: 0.0391 - dice_coef: 0.0080getting 39\n",
      "  39/1284 [..............................] - ETA: 1:09:01 - loss: 0.0393 - dice_coef: 0.0080getting 40\n",
      "loading 40\n",
      "  40/1284 [..............................] - ETA: 1:07:44 - loss: 0.0388 - dice_coef: 0.0079loading 41\n",
      "loading 42\n",
      "loading 43\n",
      "loading 44\n",
      "loading 45\n",
      "loading 46\n",
      "loading 47\n",
      "loading 48\n",
      "loading 49\n",
      "getting 41\n",
      "  41/1284 [..............................] - ETA: 1:30:36 - loss: 0.0391 - dice_coef: 0.0080getting 42\n",
      "  42/1284 [..............................] - ETA: 1:28:50 - loss: 0.0392 - dice_coef: 0.0080getting 43\n",
      "  43/1284 [>.............................] - ETA: 1:27:09 - loss: 0.0389 - dice_coef: 0.0080getting 44\n",
      "  44/1284 [>.............................] - ETA: 1:25:32 - loss: 0.0390 - dice_coef: 0.0081getting 45\n",
      "  45/1284 [>.............................] - ETA: 1:24:00 - loss: 0.0395 - dice_coef: 0.0082getting 46\n",
      "  46/1284 [>.............................] - ETA: 1:22:32 - loss: 0.0401 - dice_coef: 0.0083getting 47\n",
      "  47/1284 [>.............................] - ETA: 1:21:08 - loss: 0.0399 - dice_coef: 0.0083getting 48\n",
      "  48/1284 [>.............................] - ETA: 1:19:47 - loss: 0.0396 - dice_coef: 0.0083getting 49\n",
      "  49/1284 [>.............................] - ETA: 1:18:30 - loss: 0.0396 - dice_coef: 0.0083getting 50\n",
      "loading 50\n",
      "  50/1284 [>.............................] - ETA: 1:17:15 - loss: 0.0394 - dice_coef: 0.0083loading 51\n",
      "loading 52\n",
      "loading 53\n",
      "loading 54\n",
      "loading 55\n",
      "loading 56\n",
      "loading 57\n",
      "loading 58\n",
      "loading 59\n",
      "getting 51\n",
      "  51/1284 [>.............................] - ETA: 1:33:42 - loss: 0.0388 - dice_coef: 0.0082getting 52\n",
      "  52/1284 [>.............................] - ETA: 1:32:12 - loss: 0.0383 - dice_coef: 0.0081getting 53\n",
      "  53/1284 [>.............................] - ETA: 1:30:45 - loss: 0.0381 - dice_coef: 0.0081getting 54\n",
      "  54/1284 [>.............................] - ETA: 1:29:22 - loss: 0.0384 - dice_coef: 0.0082getting 55\n",
      "  55/1284 [>.............................] - ETA: 1:28:01 - loss: 0.0380 - dice_coef: 0.0082getting 56\n",
      "  56/1284 [>.............................] - ETA: 1:26:44 - loss: 0.0379 - dice_coef: 0.0081getting 57\n",
      "  57/1284 [>.............................] - ETA: 1:25:29 - loss: 0.0377 - dice_coef: 0.0082getting 58\n",
      "  58/1284 [>.............................] - ETA: 1:24:17 - loss: 0.0378 - dice_coef: 0.0082getting 59\n",
      "  59/1284 [>.............................] - ETA: 1:23:07 - loss: 0.0373 - dice_coef: 0.0081getting 60\n",
      "loading 60\n",
      "  60/1284 [>.............................] - ETA: 1:21:59 - loss: 0.0377 - dice_coef: 0.0083loading 61\n",
      "loading 62\n",
      "loading 63\n",
      "loading 64\n",
      "loading 65\n",
      "loading 66\n",
      "loading 67\n",
      "loading 68\n",
      "loading 69\n",
      "getting 61\n",
      "  61/1284 [>.............................] - ETA: 1:37:03 - loss: 0.0375 - dice_coef: 0.0082getting 62\n",
      "  62/1284 [>.............................] - ETA: 1:35:44 - loss: 0.0374 - dice_coef: 0.0083getting 63\n",
      "  63/1284 [>.............................] - ETA: 1:34:26 - loss: 0.0372 - dice_coef: 0.0083getting 64\n",
      "  64/1284 [>.............................] - ETA: 1:33:11 - loss: 0.0369 - dice_coef: 0.0083getting 65\n",
      "  65/1284 [>.............................] - ETA: 1:31:59 - loss: 0.0369 - dice_coef: 0.0083getting 66\n",
      "  66/1284 [>.............................] - ETA: 1:30:48 - loss: 0.0366 - dice_coef: 0.0083getting 67\n",
      "  67/1284 [>.............................] - ETA: 1:29:40 - loss: 0.0365 - dice_coef: 0.0083getting 68\n",
      "  68/1284 [>.............................] - ETA: 1:28:34 - loss: 0.0362 - dice_coef: 0.0083getting 69\n",
      "  69/1284 [>.............................] - ETA: 1:27:30 - loss: 0.0366 - dice_coef: 0.0084getting 70\n",
      "loading 70\n",
      "  70/1284 [>.............................] - ETA: 1:26:27 - loss: 0.0363 - dice_coef: 0.0083loading 71\n",
      "loading 72\n",
      "loading 73\n",
      "loading 74\n",
      "loading 75\n",
      "loading 76\n",
      "loading 77\n",
      "loading 78\n",
      "loading 79\n",
      "getting 71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN:\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#    tf.debugging.set_log_device_placement(True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#    with tf.device('/CPU:0'):\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Train the model, doing validation at the end of each epoch.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Loads the weights\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(checkpoint_path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "\n",
    "#    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "#    # Place tensors on the CPU\n",
    "#    with tf.device('/CPU:0'):\n",
    "    # Train the model, doing validation at the end of each epoch.\n",
    "    model.fit(train_set, epochs=Config.num_epochs, validation_data=valid_set, callbacks=callbacks,\n",
    "             shuffle=False)\n",
    "    \n",
    "else:\n",
    "    # Loads the weights\n",
    "    model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting, we will have to apply the **threshold**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(pred, threshold):\n",
    "    return (pred > threshold).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN:    \n",
    "    # Evaluate the model\n",
    "\n",
    "    ALL_BATCHES = True\n",
    "    BATCH_IDX = 0\n",
    "    SAMPLE_IDX = 1\n",
    "\n",
    "    if ALL_BATCHES:\n",
    "        loss, acc = model.evaluate(partial_set, verbose=2)\n",
    "    else:\n",
    "        eval_images, eval_masks = partial_set[BATCH_IDX]\n",
    "        loss, acc = model.evaluate(eval_images, eval_masks, verbose=2)\n",
    "\n",
    "    print(\"Saved model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "    predictions = model.predict(partial_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dice_coef(sample_set, pred_set, batch_size, threshold):\n",
    "    dice_coef_per_batch = np.full(len(sample_set), np.nan)\n",
    "    for idx in range(len(sample_set)):\n",
    "        x, y = sample_set[idx]\n",
    "        pred = pred_set[idx*batch_size:(idx + 1)*batch_size]\n",
    "        _coef = dice_coef(y, pred, threshold=threshold)\n",
    "        dice_coef_per_batch[idx] = _coef\n",
    "    return dice_coef_per_batch\n",
    "\n",
    "if not TRAIN:\n",
    "    _coefs = eval_dice_coef(\n",
    "        partial_set, predictions, batch_size=partial_set.batch_size, threshold=None)\n",
    "    print(f'w/o threshod: {_coefs.mean():.2%}')\n",
    "\n",
    "    _coefs = eval_dice_coef(\n",
    "        partial_set, predictions, batch_size=partial_set.batch_size, threshold=Config.threshold)\n",
    "    print(f'{Config.threshold}: {_coefs.mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(img, truth, pred):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 8))\n",
    "\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title(\"Ash Color Image\")\n",
    "\n",
    "    axs[1].imshow(truth)\n",
    "    axs[1].set_title(\"Ground Truth\")\n",
    "\n",
    "    axs[2].imshow(pred)\n",
    "    axs[2].set_title(\"Prediction\")\n",
    "\n",
    "    axs[3].imshow(img)\n",
    "    axs[3].imshow(truth, cmap='Reds', alpha=.3, interpolation='none')\n",
    "    axs[3].set_title('Contrail mask on ash color image')\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "if not TRAIN:\n",
    "    eval_images, eval_masks = partial_set[BATCH_IDX]\n",
    "    idx = SAMPLE_IDX\n",
    "    threshold = Config.threshold\n",
    "    plot_prediction(\n",
    "        eval_images[idx], eval_masks[idx], apply_threshold(predictions[idx], threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(x, fg_val=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x:  numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns: run length encoding as list\n",
    "    \"\"\"\n",
    "\n",
    "    dots = np.where(\n",
    "        x.T.flatten() == fg_val)[0]  # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def list_to_string(x):\n",
    "    \"\"\"\n",
    "    Converts list to a string representation\n",
    "    Empty list returns '-'\n",
    "    \"\"\"\n",
    "    if x: # non-empty list\n",
    "        s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")\n",
    "    else:\n",
    "        s = '-'\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recs = os.listdir(os.path.join(DATA_DIR, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'), index_col='record_id')[0:0]\n",
    "\n",
    "for test_id, pred in zip(test_ids, predictions):\n",
    "    \n",
    "    mask = apply_threshold(pred, Config.threshold)\n",
    "    \n",
    "    # notice the we're converting rec to an `int` here:\n",
    "    submission.loc[int(test_id), 'encoded_pixels'] = list_to_string(rle_encode(mask))\n",
    "    \n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Terminated', datetime.datetime.now(timezone('CET')).strftime(PRINT_TIME_FORMAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
