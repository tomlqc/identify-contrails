{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Contrails with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinstall tensorflow-io\n",
    "# to avoid the UserWarning: unable to load libtensorflow_io_plugins.so\n",
    "\n",
    "#!pip install tensorflow-io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "\n",
    "TRAIN = True\n",
    "#PLATFORM = 'kaggle'\n",
    "PLATFORM = 'gcp'\n",
    "\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cp /kaggle/input/identify-contrails/contrails_2023-07-15_15-10-49.h5 .\n",
    "#%mv weights_v8_contrails-unet_cst-lr.h5 contrails-unet_cst-lr.h5\n",
    "\n",
    "if not TRAIN:\n",
    "    checkpoint_path = 'contrails_2023-07-15_15-10-49.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from pytz import timezone\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_TIME_FORMAT = \"%Y-%m-%d %H:%M:%S %Z%z\"\n",
    "FILE_TIME_FORMAT = \"%Y-%m-%d_%H-%M-%S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 2023-07-19 18:35:58 CEST+0200\n"
     ]
    }
   ],
   "source": [
    "now_time = datetime.datetime.now(timezone('CET'))\n",
    "\n",
    "file_time_str = now_time.strftime(FILE_TIME_FORMAT)\n",
    "\n",
    "print('Started', now_time.strftime(PRINT_TIME_FORMAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num CPUs Available:  1\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/identify-contrails/notebooks'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following U-Net model is adapted from:\n",
    "- https://keras.io/examples/vision/oxford_pets_image_segmentation\n",
    "- https://www.kaggle.com/code/shashwatraman/simple-unet-baseline-train-lb-0-580"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "if PLATFORM == 'kaggle':\n",
    "\n",
    "    WORK_DIR = '/kaggle/working'  # preserved if notebook is saved\n",
    "    TEMP_DIR = '/kaggle/temp'  # just during current session\n",
    "\n",
    "    DATA_DIR = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n",
    "\n",
    "elif PLATFORM == 'gcp':\n",
    "\n",
    "    WORK_DIR = '/home/jupyter/kaggle/working'  # preserved if notebook is saved\n",
    "    TEMP_DIR = '/home/jupyter/kaggle/temp'  # just during current session\n",
    "\n",
    "    DATA_DIR = '/home/jupyter/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n",
    "    \n",
    "    %cd $WORK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paths:\n",
    "    train = os.path.join(DATA_DIR, 'train')\n",
    "    valid = os.path.join(DATA_DIR, 'validation')\n",
    "    test = os.path.join(DATA_DIR, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20529 1856 2\n"
     ]
    }
   ],
   "source": [
    "train_ids = sorted(os.listdir(Paths.train))\n",
    "valid_ids = sorted(os.listdir(Paths.valid))\n",
    "test_ids = sorted(os.listdir(Paths.test))\n",
    "print(len(train_ids), len(valid_ids), len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABI:\n",
    "    bands = {name: idx for idx, name in enumerate([\n",
    "        '08', '09', '10', '11', '12', '13', '14', '15', '16'])}\n",
    "    colors = {name: idx for idx, name in enumerate([\n",
    "        'red', 'blue', 'green', 'orange', 'purple', 'cyan', 'magenta', 'yellow', 'black'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TIMES_BEFORE = 4\n",
    "N_TIMES_AFTER = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_range(data, bounds):\n",
    "    \"\"\"Maps data to the range [0, 1].\"\"\"\n",
    "    return (data - bounds[0]) / (bounds[1] - bounds[0])\n",
    "\n",
    "_T11_BOUNDS = (243, 303)\n",
    "_CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n",
    "_TDIFF_BOUNDS = (-4, 2)\n",
    "\n",
    "def get_ash_colors(sample_id, split_dir):\n",
    "    \"\"\"\n",
    "    Based on bands: 11, 14, 15\n",
    "    \n",
    "    Args:\n",
    "        sample_id(str): The id of the example i.e. '1000216489776414077'\n",
    "        split_dir(str): The split directoryu i.e. 'test', 'train', 'val'\n",
    "    \"\"\"\n",
    "    band15 = np.load(DATA_DIR + f\"/{split_dir}/{sample_id}/band_15.npy\")\n",
    "    band14 = np.load(DATA_DIR + f\"/{split_dir}/{sample_id}/band_14.npy\")\n",
    "    band11 = np.load(DATA_DIR + f\"/{split_dir}/{sample_id}/band_11.npy\")\n",
    "\n",
    "    r = normalize_range(band15 - band14, _TDIFF_BOUNDS)\n",
    "    g = normalize_range(band14 - band11, _CLOUD_TOP_TDIFF_BOUNDS)\n",
    "    b = normalize_range(band14, _T11_BOUNDS)\n",
    "    ash_colors = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n",
    "    \n",
    "    return ash_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_mask(sample_id, split_dir):\n",
    "    masks_path = DATA_DIR + f\"/{split_dir}/{sample_id}/human_individual_masks.npy\"\n",
    "    pixel_mask = np.load(masks_path)\n",
    "    return pixel_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_mask(sample_id, split_dir):\n",
    "    masks_path = DATA_DIR + f\"/{split_dir}/{sample_id}/human_pixel_masks.npy\"\n",
    "    pixel_mask = np.load(masks_path)\n",
    "    return pixel_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some values (DEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "0.0 0.6714986\n",
      "0.0 0.8929291\n",
      "0.0 0.8980291\n"
     ]
    }
   ],
   "source": [
    "sample_id = train_ids[3]\n",
    "\n",
    "ash_colors = get_ash_colors(sample_id, 'train')[..., N_TIMES_BEFORE]\n",
    "\n",
    "print(ash_colors.shape)\n",
    "for color in range(3):\n",
    "    array = ash_colors[..., color]\n",
    "    print(array.min(), array.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 1)\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "pixel_mask = get_pixel_mask(sample_id, 'train')\n",
    "\n",
    "print(pixel_mask.shape)\n",
    "print(pixel_mask.min(), pixel_mask.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:30:33.222846Z",
     "iopub.status.busy": "2023-07-10T19:30:33.222338Z",
     "iopub.status.idle": "2023-07-10T19:30:33.228013Z",
     "shell.execute_reply": "2023-07-10T19:30:33.227016Z",
     "shell.execute_reply.started": "2023-07-10T19:30:33.222811Z"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    img_size = (256, 256)\n",
    "    \n",
    "    num_epochs = 10  # <DEVEL> else 10\n",
    "    num_classes = 1\n",
    "    batch_size = 16  # <DEVEL> else 32\n",
    "    \n",
    "    threshold = 0.40\n",
    "    \n",
    "    seed = SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/keras_recipes/reproducibility_recipes/\n",
    "\n",
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) `tensorflow` random seed\n",
    "# 3) `python` random seed\n",
    "keras.utils.set_random_seed(Config.seed)\n",
    "\n",
    "# See also:\n",
    "# tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model inspired by:\n",
    "- https://www.coursera.org/learn/advanced-computer-vision-with-tensorflow/\n",
    "- file:///D:/Courses/2023-07_Advanced-Computer-Vision-with-TensorFlow/lecture-notes/C3_W3_Image-Segmentation.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
    "    x = input_tensor\n",
    "    for i in range(2):\n",
    "        x = tf.keras.layers.SeparableConv2D(\n",
    "            filters = n_filters, kernel_size=(kernel_size, kernel_size), padding='same')(x)\n",
    "        #? kernel_initializer = 'he_normal'\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, n_filters, pool_size, dropout):\n",
    "    f = conv2d_block(inputs, n_filters=n_filters)\n",
    "    p = tf.keras.layers.MaxPooling2D(pool_size)(f)\n",
    "    p = tf.keras.layers.Dropout(dropout)(p)\n",
    "    return f, p\n",
    "\n",
    "def encoder(inputs, dropout=0.1):\n",
    "    f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=dropout)\n",
    "    f2, p2 = encoder_block(p1, n_filters=128, pool_size=(2,2), dropout=dropout)\n",
    "    f3, p3 = encoder_block(p2, n_filters=256, pool_size=(2,2), dropout=dropout)\n",
    "    f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2,2), dropout=dropout)\n",
    "    return p4, (f1, f2, f3, f4)\n",
    "\n",
    "def bottleneck(inputs):\n",
    "    bottle_neck = conv2d_block(inputs, n_filters=1024)\n",
    "    return bottle_neck\n",
    "\n",
    "def decoder_block(inputs, conv_output, n_filters, kernel_size, strides, dropout):\n",
    "    u = tf.keras.layers.Conv2DTranspose(\n",
    "        n_filters, kernel_size, strides=strides, padding = 'same')(inputs)\n",
    "    u = tf.keras.layers.BatchNormalization()(u)\n",
    "    c = tf.keras.layers.concatenate([u, conv_output])\n",
    "    c = tf.keras.layers.Dropout(dropout)(c)\n",
    "    c = conv2d_block(c, n_filters, kernel_size=3)\n",
    "    return c\n",
    "\n",
    "def decoder(inputs, convs, num_classes, dropout=0.1):\n",
    "    f1, f2, f3, f4 = convs\n",
    "    c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "    c7 = decoder_block(c6, f3, n_filters=256, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "    c8 = decoder_block(c7, f2, n_filters=128, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "    c9 = decoder_block(c8, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=dropout)\n",
    "    if num_classes == 1:\n",
    "        activation = \"sigmoid\"\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "    outputs = layers.Conv2D(num_classes, kernel_size=3, activation=activation, padding=\"same\")(c9)\n",
    "    return outputs\n",
    "\n",
    "def unet(image_size, num_classes):\n",
    "    inputs = tf.keras.layers.Input(shape=(image_size,image_size,3))\n",
    "    encoder_output, convs = encoder(inputs)\n",
    "    #model = tf.keras.Model(inputs=inputs, outputs=encoder_output)  # debug\n",
    "    bottle_neck = bottleneck(encoder_output)\n",
    "    outputs = decoder(bottle_neck, convs, num_classes)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 256, 256, 64  283        ['input_1[0][0]']                \n",
      " v2D)                           )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256, 256, 64  256        ['separable_conv2d[0][0]']       \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 256, 256, 64  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 256, 256, 64  4736       ['activation[0][0]']             \n",
      " onv2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 256, 256, 64  256        ['separable_conv2d_1[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 256, 256, 64  0           ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 128, 128, 64  0           ['activation_1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128, 128, 64  0           ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 128, 128, 12  8896       ['dropout[0][0]']                \n",
      " onv2D)                         8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128, 128, 12  512        ['separable_conv2d_2[0][0]']     \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_2[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 128, 128, 12  17664      ['activation_2[0][0]']           \n",
      " onv2D)                         8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128, 128, 12  512        ['separable_conv2d_3[0][0]']     \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_3[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0          ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64, 64, 128)  0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 64, 64, 256)  34176      ['dropout_1[0][0]']              \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 64, 256)  1024       ['separable_conv2d_4[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 64, 64, 256)  68096      ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64, 64, 256)  1024       ['separable_conv2d_5[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0          ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32, 32, 256)  0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " separable_conv2d_6 (SeparableC  (None, 32, 32, 512)  133888     ['dropout_2[0][0]']              \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 512)  2048       ['separable_conv2d_6[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 512)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_7 (SeparableC  (None, 32, 32, 512)  267264     ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 512)  2048       ['separable_conv2d_7[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 512)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0          ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 16, 16, 512)  0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " separable_conv2d_8 (SeparableC  (None, 16, 16, 1024  529920     ['dropout_3[0][0]']              \n",
      " onv2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 1024  4096       ['separable_conv2d_8[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 1024  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_9 (SeparableC  (None, 16, 16, 1024  1058816    ['activation_8[0][0]']           \n",
      " onv2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 1024  4096       ['separable_conv2d_9[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 1024  0           ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 32, 32, 512)  4719104    ['activation_9[0][0]']           \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_transpose[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 1024  0           ['batch_normalization_10[0][0]', \n",
      "                                )                                 'activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 32, 32, 1024  0           ['concatenate[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_10 (Separable  (None, 32, 32, 512)  534016     ['dropout_4[0][0]']              \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 512)  2048       ['separable_conv2d_10[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_11 (Separable  (None, 32, 32, 512)  267264     ['activation_10[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 512)  2048       ['separable_conv2d_11[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 256)  1179904    ['activation_11[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_transpose_1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 512)  0           ['batch_normalization_13[0][0]', \n",
      "                                                                  'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64, 64, 512)  0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " separable_conv2d_12 (Separable  (None, 64, 64, 256)  135936     ['dropout_5[0][0]']              \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 64, 64, 256)  1024       ['separable_conv2d_12[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_13 (Separable  (None, 64, 64, 256)  68096      ['activation_12[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 64, 64, 256)  1024       ['separable_conv2d_13[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 12  295040     ['activation_13[0][0]']          \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 128, 128, 12  512        ['conv2d_transpose_2[0][0]']     \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 128, 25  0           ['batch_normalization_16[0][0]', \n",
      "                                6)                                'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 128, 128, 25  0           ['concatenate_2[0][0]']          \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_14 (Separable  (None, 128, 128, 12  35200      ['dropout_6[0][0]']              \n",
      " Conv2D)                        8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 128, 128, 12  512        ['separable_conv2d_14[0][0]']    \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_17[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_15 (Separable  (None, 128, 128, 12  17664      ['activation_14[0][0]']          \n",
      " Conv2D)                        8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 128, 128, 12  512        ['separable_conv2d_15[0][0]']    \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_18[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 256, 256, 64  73792      ['activation_15[0][0]']          \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 256, 256, 64  256        ['conv2d_transpose_3[0][0]']     \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 256, 256, 12  0           ['batch_normalization_19[0][0]', \n",
      "                                8)                                'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 256, 256, 12  0           ['concatenate_3[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_16 (Separable  (None, 256, 256, 64  9408       ['dropout_7[0][0]']              \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 256, 256, 64  256        ['separable_conv2d_16[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_20[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_17 (Separable  (None, 256, 256, 64  4736       ['activation_16[0][0]']          \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 256, 256, 64  256        ['separable_conv2d_17[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_21[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 256, 256, 1)  577         ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,491,868\n",
      "Trainable params: 9,478,172\n",
      "Non-trainable params: 13,696\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "#model = get_model(Config.img_size, Config.num_classes)\n",
    "model = unet(image_size=256, num_classes=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:30:33.222846Z",
     "iopub.status.busy": "2023-07-10T19:30:33.222338Z",
     "iopub.status.idle": "2023-07-10T19:30:33.228013Z",
     "shell.execute_reply": "2023-07-10T19:30:33.227016Z",
     "shell.execute_reply.started": "2023-07-10T19:30:33.222811Z"
    }
   },
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = None  # None to take all\n",
    "N_PARTIAL = 128  # 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AshColorSingleFrames(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, sample_ids, split_dir, n_samples=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.split_dir = split_dir\n",
    "        self.sample_ids = sample_ids[:n_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.sample_ids) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_sample_ids = self.sample_ids[i : i + self.batch_size]\n",
    "        \n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, sample_id in enumerate(batch_sample_ids):\n",
    "            img = get_ash_colors(sample_id, self.split_dir)\n",
    "            x[j] = img[..., N_TIMES_BEFORE]\n",
    "\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        if self.split_dir != 'test':\n",
    "            for j, sample_id in enumerate(batch_sample_ids):\n",
    "                img = get_pixel_mask(sample_id, self.split_dir)\n",
    "                y[j] = img\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 1284\n"
     ]
    }
   ],
   "source": [
    "train_set = AshColorSingleFrames(Config.batch_size, Config.img_size, train_ids, 'train',\n",
    "                                 n_samples=N_SAMPLES)  # <DEVEL>\n",
    "print('number of batches:', len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 116\n"
     ]
    }
   ],
   "source": [
    "valid_set = AshColorSingleFrames(Config.batch_size, Config.img_size, valid_ids, 'validation',\n",
    "                                 n_samples=N_SAMPLES)  # <DEVEL>\n",
    "print('number of batches:', len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 8\n"
     ]
    }
   ],
   "source": [
    "partial_set = AshColorSingleFrames(Config.batch_size, Config.img_size, valid_ids, 'validation',\n",
    "                                 n_samples=N_PARTIAL)  # <DEVEL>\n",
    "print('number of batches:', len(partial_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 1\n"
     ]
    }
   ],
   "source": [
    "test_set = AshColorSingleFrames(Config.batch_size, Config.img_size, test_ids, 'test')\n",
    "print('number of batches:', len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check batch dimensions (x, y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 256, 256, 3), (16, 256, 256, 1))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0].shape, train_set[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dice_coef` adapted from:\n",
    "- https://stackoverflow.com/questions/72195156/correct-implementation-of-dice-loss-in-tensorflow-keras\n",
    "- https://www.kaggle.com/code/shashwatraman/simple-unet-baseline-train-lb-0-580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=0.001, threshold=None):\n",
    "    y_true_f = backend.flatten(tf.cast(y_true, tf.float32))\n",
    "    y_pred_f = backend.flatten(tf.cast(y_pred, tf.float32))\n",
    "    # ValueError: No gradients provided for any variable\n",
    "    if threshold is not None:\n",
    "        y_pred_f = backend.flatten(\n",
    "            tf.cast(tf.math.greater(tf.cast(y_pred, tf.float32), threshold), tf.float32))\n",
    "    intersection = backend.sum(y_true_f * y_pred_f)\n",
    "    dice = (2. * intersection + smooth) / (backend.sum(y_true_f) + backend.sum(y_pred_f) + smooth)\n",
    "    return dice\n",
    "\n",
    "def threshold_dice_coef(y_true, y_pred, smooth=0.001):\n",
    "    return dice_coef(y_true, y_pred, smooth=smooth, threshold=Config.threshold)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check `dice_coef()` on one of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample_id = train_ids[3]\n",
    "\n",
    "merged_mask = get_pixel_mask(sample_id, 'train')\n",
    "indiv_masks = get_individual_mask(sample_id, 'train')\n",
    "\n",
    "print(dice_coef(tf.convert_to_tensor(merged_mask),\n",
    "                tf.convert_to_tensor(merged_mask)))\n",
    "for idv in range(6):\n",
    "    print(dice_coef(tf.convert_to_tensor(merged_mask),\n",
    "                    tf.convert_to_tensor(indiv_masks[..., idv])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate scheduler:\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay - warmup only from v2.13.1 on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint file: contrails_2023-07-19_18-35-58.h5\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    checkpoint_path = f\"contrails_{file_time_str}.h5\"\n",
    "\n",
    "print(f'checkpoint file: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "decay_steps = len(train_set)\n",
    "decay_rate = 0.7\n",
    "\n",
    "cos_scheduler = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate, decay_steps)\n",
    "\n",
    "exp_scheduler = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the model for training.\n",
    "# We use the \"sparse\" version of categorical_crossentropy\n",
    "# because our target data is integers.\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[dice_coef])\n",
    "#model.compile(optimizer=\"adam\", loss=dice_loss, metrics=[dice_coef])\n",
    "\n",
    "callbacks = [\n",
    "    #keras.callbacks.LearningRateScheduler(exp_scheduler),\n",
    "    keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # Train the model, doing validation at the end of each epoch.\n",
    "    model.fit(train_set, epochs=Config.num_epochs, validation_data=valid_set, callbacks=callbacks,\n",
    "             workers=4, use_multiprocessing=True)\n",
    "else:\n",
    "    # Loads the weights\n",
    "    model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting, we will have to apply the **threshold**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(pred, threshold):\n",
    "    return (pred > threshold).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN:    \n",
    "    # Evaluate the model\n",
    "\n",
    "    ALL_BATCHES = True\n",
    "    BATCH_IDX = 0\n",
    "    SAMPLE_IDX = 1\n",
    "\n",
    "    if ALL_BATCHES:\n",
    "        loss, acc = model.evaluate(partial_set, verbose=2)\n",
    "    else:\n",
    "        eval_images, eval_masks = partial_set[BATCH_IDX]\n",
    "        loss, acc = model.evaluate(eval_images, eval_masks, verbose=2)\n",
    "\n",
    "    print(\"Saved model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "    predictions = model.predict(partial_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dice_coef(sample_set, pred_set, batch_size, threshold):\n",
    "    dice_coef_per_batch = np.full(len(sample_set), np.nan)\n",
    "    for idx in range(len(sample_set)):\n",
    "        x, y = sample_set[idx]\n",
    "        pred = pred_set[idx*batch_size:(idx + 1)*batch_size]\n",
    "        _coef = dice_coef(y, pred, threshold=threshold)\n",
    "        dice_coef_per_batch[idx] = _coef\n",
    "    return dice_coef_per_batch\n",
    "\n",
    "if not TRAIN:\n",
    "    _coefs = eval_dice_coef(\n",
    "        partial_set, predictions, batch_size=partial_set.batch_size, threshold=None)\n",
    "    print(f'w/o threshod: {_coefs.mean():.2%}')\n",
    "\n",
    "    _coefs = eval_dice_coef(\n",
    "        partial_set, predictions, batch_size=partial_set.batch_size, threshold=Config.threshold)\n",
    "    print(f'{Config.threshold}: {_coefs.mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(img, truth, pred):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 8))\n",
    "\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title(\"Ash Color Image\")\n",
    "\n",
    "    axs[1].imshow(truth)\n",
    "    axs[1].set_title(\"Ground Truth\")\n",
    "\n",
    "    axs[2].imshow(pred)\n",
    "    axs[2].set_title(\"Prediction\")\n",
    "\n",
    "    axs[3].imshow(img)\n",
    "    axs[3].imshow(truth, cmap='Reds', alpha=.3, interpolation='none')\n",
    "    axs[3].set_title('Contrail mask on ash color image')\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "if not TRAIN:\n",
    "    eval_images, eval_masks = partial_set[BATCH_IDX]\n",
    "    idx = SAMPLE_IDX\n",
    "    threshold = Config.threshold\n",
    "    plot_prediction(\n",
    "        eval_images[idx], eval_masks[idx], apply_threshold(predictions[idx], threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(x, fg_val=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x:  numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns: run length encoding as list\n",
    "    \"\"\"\n",
    "\n",
    "    dots = np.where(\n",
    "        x.T.flatten() == fg_val)[0]  # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def list_to_string(x):\n",
    "    \"\"\"\n",
    "    Converts list to a string representation\n",
    "    Empty list returns '-'\n",
    "    \"\"\"\n",
    "    if x: # non-empty list\n",
    "        s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")\n",
    "    else:\n",
    "        s = '-'\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recs = os.listdir(os.path.join(DATA_DIR, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'), index_col='record_id')[0:0]\n",
    "\n",
    "for test_id, pred in zip(test_ids, predictions):\n",
    "    \n",
    "    mask = apply_threshold(pred, Config.threshold)\n",
    "    \n",
    "    # notice the we're converting rec to an `int` here:\n",
    "    submission.loc[int(test_id), 'encoded_pixels'] = list_to_string(rle_encode(mask))\n",
    "    \n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Terminated', datetime.datetime.now(timezone('CET')).strftime(PRINT_TIME_FORMAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
